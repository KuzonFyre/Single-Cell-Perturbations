{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KuzonFyre/Single-Cell-Perturbations/blob/main/Copy_of_Model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "zuop4MwUioMo",
        "outputId": "25f840da-2394-4308-894e-5d73bc887866",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import torch.nn as nn\n",
        "import torch\n",
        "import os\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import pandas as pd\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "scrolled": false,
        "id": "ZK-sKvdOioMu"
      },
      "outputs": [],
      "source": [
        "#Read the data into a pandas data frame\n",
        "df = pd.read_parquet(\"/content/drive/MyDrive/de_train.parquet\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PzEEIPdsjIf9"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "01lhcycMioMw"
      },
      "outputs": [],
      "source": [
        "#normalizes data between -1 and 1\n",
        "#returns normalized data and the factors used to normalzie\n",
        "def normalize(df):\n",
        "    min = df.min()\n",
        "    max=df.max()\n",
        "    df_normalized = (df - min) / (max-min)\n",
        "    return df_normalized, min.reset_index(drop=True), max.reset_index(drop=True)\n",
        "\n",
        "\n",
        "#unnormalize\n",
        "def unnormalize(normalized_df, min, max):\n",
        "    return min + normalized_df*(max-min)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "zgp5WIpaioMx"
      },
      "outputs": [],
      "source": [
        "############################DATA PREP###############################\n",
        "\n",
        "\n",
        "\n",
        "####One hot incodes inputs####\n",
        "\n",
        "# Initialize the OneHotEncoder\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "\n",
        "# Fit and transform the data\n",
        "cells = df[\"cell_type\"].values.reshape(-1, 1)\n",
        "hot_cells = encoder.fit_transform(cells)\n",
        "cell_mapping = encoder.categories_[0]\n",
        "\n",
        "compounds = df['sm_name'].values.reshape(-1, 1)\n",
        "hot_compounds = encoder.fit_transform(compounds)\n",
        "compound_mapping = encoder.categories_[0]\n",
        "\n",
        "#Puts together inputs\n",
        "inputs = np.hstack((hot_cells, hot_compounds))\n",
        "inputs_df = pd.DataFrame(data = inputs)\n",
        "\n",
        "\n",
        "####Normalizes Outputs####\n",
        "\n",
        "outputs = df.loc[:, 'A1BG':'ZZEF1']\n",
        "outputs_norm_df, norm_min, norm_max = normalize(outputs)\n",
        "\n",
        "\n",
        "####Puts Inputs and Outputs Together####\n",
        "prepped_df = pd.concat([inputs_df, outputs_norm_df], axis=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# X, y = train_test_split(prepped_df,train_size=.2, random_state=3)"
      ],
      "metadata": {
        "id": "ciNhEyGqp3y_"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Qe8rZpdlioMx"
      },
      "outputs": [],
      "source": [
        "#Break into training and validation and split inputs from outputs\n",
        "val, train = train_test_split(prepped_df, train_size=.2, random_state=3)\n",
        "trainIn_df = train.loc[:, 0:151]\n",
        "trainOut_df = train.loc[:, 'A1BG':'ZZEF1']\n",
        "valIn_df = val.loc[:,0:151]\n",
        "valOut_df = val.loc[:, 'A1BG':'ZZEF1']\n",
        "\n",
        "#Transforms data frames into tensors\n",
        "trainIn_t = torch.tensor(trainIn_df.values)\n",
        "trainOut_t = torch.tensor(trainOut_df.values)\n",
        "valIn_t = torch.tensor(valIn_df.values)\n",
        "valOut_t = torch.tensor(valOut_df.values)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xhuBSpWCioM8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "65b000c7-6531-4621-fdb3-ac8786b6e4fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/10], Loss: 0.0427\n",
            "Epoch [2/10], Loss: 0.0058\n",
            "Epoch [3/10], Loss: 0.0048\n",
            "Epoch [4/10], Loss: 0.0047\n",
            "Epoch [5/10], Loss: 0.0044\n",
            "Epoch [6/10], Loss: 0.0043\n",
            "Epoch [7/10], Loss: 0.0045\n",
            "Epoch [8/10], Loss: 0.0039\n",
            "Epoch [9/10], Loss: 0.0034\n",
            "Epoch [10/10], Loss: 0.0030\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "class MLPRegressor(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, output_size):\n",
        "        super(MLPRegressor, self).__init__()\n",
        "        layers = []\n",
        "        all_sizes = [input_size] + hidden_sizes + [output_size]\n",
        "\n",
        "        for i in range(len(all_sizes) - 1):\n",
        "            layers.append(nn.Linear(all_sizes[i], all_sizes[i + 1]))\n",
        "            if i < len(all_sizes) - 2:  # No activation on the last layer\n",
        "                layers.append(nn.ReLU())\n",
        "\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "\n",
        "# Define dataset and dataloader for batch processing\n",
        "train_dataset = TensorDataset(trainIn_t, trainOut_t)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=20, shuffle=True)\n",
        "\n",
        "# Define the model\n",
        "input_size = 152\n",
        "hidden_sizes = [1024, 512]  # You can adjust these sizes and add more layers if needed\n",
        "output_size = 18211\n",
        "model = MLPRegressor(input_size, hidden_sizes, output_size).double()\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "model.to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "loss_fn = nn.MSELoss()  # Use MSELoss for regression\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "def train(num_epochs):\n",
        "    model.train()\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_losses = []\n",
        "        for batch_idx, (X_batch, y_batch) in enumerate(train_loader):\n",
        "            # Move to device\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(X_batch)\n",
        "            loss = loss_fn(outputs, y_batch)\n",
        "\n",
        "            # Backward and optimize\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "            epoch_losses.append(loss.item())\n",
        "\n",
        "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {np.mean(epoch_losses):.4f}\")\n",
        "\n",
        "# Call the train function\n",
        "train(10)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "p4Z2feZCioM9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# #Train model (epochs, mini batch size)\n",
        "# train(20,20)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8zbnZv0BioM-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 331
        },
        "outputId": "0d3daf7d-d1b1-42e4-a0e9-5c4c0c989566"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-9-e4f1b444be86>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34mf\"Deviation: {100*accuracy}%    RSS: {RSS}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m \u001b[0mdeviationtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-9-e4f1b444be86>\u001b[0m in \u001b[0;36mdeviationtrain\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mdeviationtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainIn_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mtrainOut_t\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mtrainOut_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'model1' is not defined"
          ]
        }
      ],
      "source": [
        "# # compute accuracy on training set\n",
        "# def deviationtrain():\n",
        "#     with torch.no_grad():\n",
        "#         y_pred = model1(trainIn_t)\n",
        "\n",
        "#     accuracy = (abs((y_pred-trainOut_t)).sum()/trainOut_t.sum())\n",
        "#     RSS = ((y_pred-trainOut_t)**2).float().sum()\n",
        "#     return (f\"Deviation: {100*accuracy}%    RSS: {RSS}\")\n",
        "\n",
        "# deviationtrain()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "VEf0-I22ioM_"
      },
      "outputs": [],
      "source": [
        "def hot_encode(cell,compound):\n",
        "    cell_vec = np.zeros(cell_mapping.size)\n",
        "    cell_dict = {value: index for index, value in enumerate(cell_mapping)}\n",
        "\n",
        "    compound_vec = np.zeros(compound_mapping.size)\n",
        "    compound_dict = {value: index for index, value in enumerate(compound_mapping)}\n",
        "\n",
        "\n",
        "    cell_vec[cell_dict[cell]]=1\n",
        "    compound_vec[compound_dict[compound]]=1\n",
        "    vector = np.concatenate((cell_vec, compound_vec), axis = 0)\n",
        "    tensor = torch.from_numpy(vector)\n",
        "    return tensor\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "B4Zfs_FOioNA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "263abc53-2f9b-45ef-db77-b8c966b42abb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([0.1524, 0.2396, 0.5161,  ..., 0.4467, 0.4590, 0.5669],\n",
              "       dtype=torch.float64, grad_fn=<ViewBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "model(hot_encode(\"NK cells\", \"Clotrimazole\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "K-Lg8JkYioNC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "574ad9c1-34d8-4fd6-9571-0f1c567d20f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0        0.603001\n",
            "1        0.361692\n",
            "2        0.406551\n",
            "3        0.128234\n",
            "4        0.369398\n",
            "           ...   \n",
            "18206    0.743457\n",
            "18207    0.128856\n",
            "18208    0.241062\n",
            "18209   -0.296729\n",
            "18210   -0.083388\n",
            "Length: 18211, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "def get_expression(cell_type, compound_name):\n",
        "    tensor = model(hot_encode(cell_type,compound_name))\n",
        "    np_array = tensor.detach().numpy()\n",
        "    df = pd.DataFrame(np_array)\n",
        "    return unnormalize(df[0],norm_min,norm_max)\n",
        "\n",
        "\n",
        "df = get_expression(\"NK cells\", \"Clotrimazole\")\n",
        "print(df)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "jj2k9MvzioND"
      },
      "outputs": [],
      "source": [
        "# Read the sample submission and test set ID map\n",
        "sample_submission = pd.read_csv(\"/content/drive/MyDrive/sample_submission.csv\")\n",
        "testDf = pd.read_csv(\"/content/drive/MyDrive/id_map.csv\")\n",
        "\n",
        "# Initialize an empty list to collect the predicted values\n",
        "predicted_values = []\n",
        "\n",
        "# Loop through the test set to get the predicted values\n",
        "for idx, row in testDf.iterrows():\n",
        "    cell_type = row['cell_type']\n",
        "    sm_name = row['sm_name']\n",
        "\n",
        "    # Call your get_expression method here\n",
        "    expression_values = get_expression(cell_type, sm_name)\n",
        "\n",
        "    # Append the values to the list\n",
        "    predicted_values.append(expression_values)\n",
        "\n",
        "# Convert the list of predicted values to a numpy array\n",
        "predicted_values_array = np.array(predicted_values)\n",
        "\n",
        "# Replace the values in the sample submission DataFrame\n",
        "sample_submission.iloc[:, 1:] = predicted_values_array\n",
        "\n",
        "# Save the DataFrame to a new CSV file\n",
        "sample_submission.to_csv(\"/content/drive/MyDrive/my_submission.csv\", index=False)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8AolqvXioND"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    },
    "vscode": {
      "interpreter": {
        "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
      }
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}