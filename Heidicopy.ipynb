{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Read the data into a pandas data frame \n",
    "df = pd.read_parquet(\"de_train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdFMCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The substructure 'c1ccccc1' is found 0 times in the molecule 'CCOCC'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def count_substructure_occurrences(target_smiles, query_smiles):\n",
    "    try:\n",
    "        # Convert SMILES to RDKit molecules\n",
    "        target_mol = Chem.MolFromSmiles(target_smiles)\n",
    "        query_mol = Chem.MolFromSmiles(query_smiles)\n",
    "\n",
    "        # Check if the molecules are valid\n",
    "        if target_mol is None or query_mol is None:\n",
    "            print(\"Error: Invalid SMILES.\")\n",
    "            return None\n",
    "\n",
    "        # Use SubstructMatch to find occurrences\n",
    "        occurrences = target_mol.GetSubstructMatches(query_mol)\n",
    "\n",
    "        return len(occurrences)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "target_smiles = \"CCOCC\"\n",
    "query_smiles = \"c1ccccc1\"\n",
    "occurrence_count = count_substructure_occurrences(target_smiles, query_smiles)\n",
    "\n",
    "if occurrence_count is not None:\n",
    "    print(f\"The substructure '{query_smiles}' is found {occurrence_count} times in the molecule '{target_smiles}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "functional_groups  = [\"O\", \"C=O\", \"N\", \"C(=O)N\", \"C(=O)O\", \"N=O\", \"S\", \"P([O])([O])\", \"S(=O)([O])([O])\", \"C=C\", \"C#C\", \"c1ccccc1\", \"F\", \"Cl\", \"Br\", \"I\", \"C\" ]\n",
    "functional_column_names = [(i, 'int') for i in functional_groups]\n",
    "SMILES = df[\"SMILES\"].tolist()\n",
    "\n",
    "functional_groups_block = [[count_substructure_occurrences(i,j) for j in functional_groups ] for i in SMILES]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here a few entries from the SMILES column: \n",
      "0               Clc1ccccc1C(c1ccccc1)(c1ccccc1)n1ccnc1\n",
      "1               Clc1ccccc1C(c1ccccc1)(c1ccccc1)n1ccnc1\n",
      "2               Clc1ccccc1C(c1ccccc1)(c1ccccc1)n1ccnc1\n",
      "3               Clc1ccccc1C(c1ccccc1)(c1ccccc1)n1ccnc1\n",
      "4    C[C@@H]1C[C@H]2[C@@H]3CCC4=CC(=O)C=C[C@]4(C)[C...\n",
      "5    C[C@@H]1C[C@H]2[C@@H]3CCC4=CC(=O)C=C[C@]4(C)[C...\n",
      "6    C[C@@H]1C[C@H]2[C@@H]3CCC4=CC(=O)C=C[C@]4(C)[C...\n",
      "7    C[C@@H]1C[C@H]2[C@@H]3CCC4=CC(=O)C=C[C@]4(C)[C...\n",
      "8    CC[C@H](Nc1ncnc2[nH]cnc12)c1nc2cccc(F)c2c(=O)n...\n",
      "9    CC[C@H](Nc1ncnc2[nH]cnc12)c1nc2cccc(F)c2c(=O)n...\n",
      "Name: SMILES, dtype: object.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(f\"Here a few entries from the SMILES column: \\n{df['SMILES'].head(10)}.\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(column_names, data):\n",
    "    df = pd.DataFrame(data, columns=[name for name, _ in column_names])\n",
    "    return df\n",
    "\n",
    "functional_group_df = create_dataframe(functional_column_names, functional_groups_block)\n",
    "\n",
    "sm_names = df[\"sm_name\"].tolist\n",
    "functional_group_dict= {key: value for key, value in zip(sm_names, values)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizes data between -1 and 1\n",
    "#returns normalized data and the factors used to normalzie \n",
    "def normalize(df):\n",
    "    min = df.min()\n",
    "    max=df.max()\n",
    "    df_normalized = (df - min) / (max-min)\n",
    "    return df_normalized, min.reset_index(drop=True), max.reset_index(drop=True)\n",
    "\n",
    "\n",
    "#unnormalize\n",
    "def unnormalize(normalized_df, min, max):\n",
    "    return min + normalized_df*(max-min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "############################DATA PREP###############################\n",
    "\n",
    "\n",
    "\n",
    "####One hot incodes inputs####\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Fit and transform the data\n",
    "cells = df[\"cell_type\"].values.reshape(-1, 1)\n",
    "hot_cells = encoder.fit_transform(cells)\n",
    "cell_mapping = encoder.categories_[0]\n",
    "\n",
    "compounds = df['sm_name'].values.reshape(-1, 1)\n",
    "hot_compounds = encoder.fit_transform(compounds)\n",
    "compound_mapping = encoder.categories_[0]\n",
    "\n",
    "#Puts together inputs\n",
    "hot_encoded = np.hstack((hot_cells, hot_compounds))\n",
    "hot_encoded_df = pd.DataFrame(data = hot_encoded)\n",
    "inputs_df = pd.concat([hot_encoded_df, functional_group_df], axis=1)\n",
    "\n",
    "####Normalizes Outputs####\n",
    "\n",
    "outputs = df.loc[:, 'A1BG':'ZZEF1']\n",
    "outputs_norm_df, norm_min, norm_max = normalize(outputs)\n",
    "\n",
    "\n",
    "####Puts Inputs and Outputs Together####\n",
    "prepped_df = pd.concat([inputs_df, outputs_norm_df], axis=1)\n",
    "\n",
    "print(type(hot_cells))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "#Break into training and validation and split inputs from outputs\n",
    "val, train = train_test_split(prepped_df, train_size=.2, random_state=3)\n",
    "trainIn_df = train.loc[:, 0:\"C\"]\n",
    "trainOut_df = train.loc[:, 'A1BG':'ZZEF1']\n",
    "valIn_df = val.loc[:,0:\"C\"]\n",
    "valOut_df = val.loc[:, 'A1BG':'ZZEF1']\n",
    "\n",
    "#Transforms data frames into tensors\n",
    "trainIn_t = torch.tensor(trainIn_df.values).float()\n",
    "trainOut_t = torch.tensor(trainOut_df.values).float()\n",
    "valIn_t = torch.tensor(valIn_df.values).float()\n",
    "valOut_t = torch.tensor(valOut_df.values).float()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomNetwork, self).__init__()\n",
    "        # Define the layers\n",
    "        self.layer1 = nn.Linear(169, 512)  # First hidden layer\n",
    "        self.batch_norm1 = nn.BatchNorm1d(512)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "\n",
    "        self.layer2 = nn.Linear(512, 1024) # Second hidden layer\n",
    "        self.batch_norm2 = nn.BatchNorm1d(1024)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        self.layer3 = nn.Linear(1024, 512) # Third hidden layer\n",
    "        self.batch_norm3 = nn.BatchNorm1d(512)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "\n",
    "        self.layer4 = nn.Linear(512, 256) # Fourth hidden layer\n",
    "        self.batch_norm4 = nn.BatchNorm1d(256)\n",
    "        self.dropout4 = nn.Dropout(0.3)\n",
    "\n",
    "        self.output_layer = nn.Linear(256, 18211) # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = F.relu(self.batch_norm1(self.layer1(x)))\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = F.relu(self.batch_norm2(self.layer2(x)))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = F.relu(self.batch_norm3(self.layer3(x)))\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = F.relu(self.batch_norm4(self.layer4(x)))\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "def train_model(model, trainIn_t, trainOut_t, valIn_t, valOut_t, batch_size, num_epochs):\n",
    "    criterion = nn.BCEWithLogitsLoss()  # Adjust according to your needs\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.3, verbose=True)\n",
    "\n",
    "    num_samples = trainIn_t.shape[0]\n",
    "    num_batches = int(np.ceil(num_samples / batch_size))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        # Shuffle the data at the beginning of each epoch\n",
    "        permutation = torch.randperm(num_samples)\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min(start_idx + batch_size, num_samples)\n",
    "            indices = permutation[start_idx:end_idx]\n",
    "\n",
    "            batch_trainIn = trainIn_t[indices]\n",
    "            batch_trainOut = trainOut_t[indices]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_trainIn)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, batch_trainOut)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item() * len(indices)\n",
    "\n",
    "        epoch_loss /= num_samples\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_output_predictions = model(valIn_t)\n",
    "            val_loss = criterion(val_output_predictions, valOut_t)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 0.6758, Validation Loss: 0.6881\n",
      "Epoch 2/100, Training Loss: 0.6006, Validation Loss: 0.6145\n",
      "Epoch 3/100, Training Loss: 0.5599, Validation Loss: 0.5566\n",
      "Epoch 4/100, Training Loss: 0.5431, Validation Loss: 0.5379\n",
      "Epoch 5/100, Training Loss: 0.5371, Validation Loss: 0.5324\n",
      "Epoch 6/100, Training Loss: 0.5338, Validation Loss: 0.5323\n",
      "Epoch 7/100, Training Loss: 0.5310, Validation Loss: 0.5307\n",
      "Epoch 8/100, Training Loss: 0.5294, Validation Loss: 0.5295\n",
      "Epoch 9/100, Training Loss: 0.5287, Validation Loss: 0.5290\n",
      "Epoch 10/100, Training Loss: 0.5274, Validation Loss: 0.5284\n",
      "Epoch 11/100, Training Loss: 0.5277, Validation Loss: 0.5281\n",
      "Epoch 12/100, Training Loss: 0.5264, Validation Loss: 0.5279\n",
      "Epoch 13/100, Training Loss: 0.5257, Validation Loss: 0.5278\n",
      "Epoch 14/100, Training Loss: 0.5261, Validation Loss: 0.5278\n",
      "Epoch 15/100, Training Loss: 0.5254, Validation Loss: 0.5282\n",
      "Epoch 16/100, Training Loss: 0.5256, Validation Loss: 0.5282\n",
      "Epoch 17/100, Training Loss: 0.5249, Validation Loss: 0.5283\n",
      "Epoch 18/100, Training Loss: 0.5249, Validation Loss: 0.5283\n",
      "Epoch 00019: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 19/100, Training Loss: 0.5256, Validation Loss: 0.5283\n",
      "Epoch 20/100, Training Loss: 0.5245, Validation Loss: 0.5282\n",
      "Epoch 21/100, Training Loss: 0.5244, Validation Loss: 0.5282\n",
      "Epoch 22/100, Training Loss: 0.5246, Validation Loss: 0.5282\n",
      "Epoch 23/100, Training Loss: 0.5247, Validation Loss: 0.5283\n",
      "Epoch 24/100, Training Loss: 0.5245, Validation Loss: 0.5284\n",
      "Epoch 00025: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 25/100, Training Loss: 0.5240, Validation Loss: 0.5286\n",
      "Epoch 26/100, Training Loss: 0.5242, Validation Loss: 0.5286\n",
      "Epoch 27/100, Training Loss: 0.5242, Validation Loss: 0.5286\n",
      "Epoch 28/100, Training Loss: 0.5244, Validation Loss: 0.5286\n",
      "Epoch 29/100, Training Loss: 0.5244, Validation Loss: 0.5286\n",
      "Epoch 30/100, Training Loss: 0.5246, Validation Loss: 0.5286\n",
      "Epoch 00031: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 31/100, Training Loss: 0.5245, Validation Loss: 0.5286\n",
      "Epoch 32/100, Training Loss: 0.5241, Validation Loss: 0.5286\n",
      "Epoch 33/100, Training Loss: 0.5243, Validation Loss: 0.5285\n",
      "Epoch 34/100, Training Loss: 0.5242, Validation Loss: 0.5286\n",
      "Epoch 35/100, Training Loss: 0.5243, Validation Loss: 0.5286\n",
      "Epoch 36/100, Training Loss: 0.5241, Validation Loss: 0.5286\n",
      "Epoch 00037: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 37/100, Training Loss: 0.5242, Validation Loss: 0.5286\n",
      "Epoch 38/100, Training Loss: 0.5242, Validation Loss: 0.5286\n",
      "Epoch 39/100, Training Loss: 0.5243, Validation Loss: 0.5286\n",
      "Epoch 40/100, Training Loss: 0.5240, Validation Loss: 0.5286\n",
      "Epoch 41/100, Training Loss: 0.5241, Validation Loss: 0.5286\n",
      "Epoch 42/100, Training Loss: 0.5245, Validation Loss: 0.5286\n",
      "Epoch 00043: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 43/100, Training Loss: 0.5238, Validation Loss: 0.5286\n",
      "Epoch 44/100, Training Loss: 0.5239, Validation Loss: 0.5285\n",
      "Epoch 45/100, Training Loss: 0.5240, Validation Loss: 0.5286\n",
      "Epoch 46/100, Training Loss: 0.5241, Validation Loss: 0.5286\n",
      "Epoch 47/100, Training Loss: 0.5240, Validation Loss: 0.5285\n",
      "Epoch 48/100, Training Loss: 0.5238, Validation Loss: 0.5286\n",
      "Epoch 00049: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 49/100, Training Loss: 0.5241, Validation Loss: 0.5286\n",
      "Epoch 50/100, Training Loss: 0.5244, Validation Loss: 0.5286\n",
      "Epoch 51/100, Training Loss: 0.5238, Validation Loss: 0.5286\n",
      "Epoch 52/100, Training Loss: 0.5239, Validation Loss: 0.5286\n",
      "Epoch 53/100, Training Loss: 0.5244, Validation Loss: 0.5285\n",
      "Epoch 54/100, Training Loss: 0.5243, Validation Loss: 0.5285\n",
      "Epoch 00055: reducing learning rate of group 0 to 2.1870e-07.\n",
      "Epoch 55/100, Training Loss: 0.5242, Validation Loss: 0.5286\n",
      "Epoch 56/100, Training Loss: 0.5238, Validation Loss: 0.5285\n",
      "Epoch 57/100, Training Loss: 0.5249, Validation Loss: 0.5286\n",
      "Epoch 58/100, Training Loss: 0.5240, Validation Loss: 0.5286\n",
      "Epoch 59/100, Training Loss: 0.5246, Validation Loss: 0.5286\n",
      "Epoch 60/100, Training Loss: 0.5239, Validation Loss: 0.5286\n",
      "Epoch 00061: reducing learning rate of group 0 to 6.5610e-08.\n",
      "Epoch 61/100, Training Loss: 0.5240, Validation Loss: 0.5286\n",
      "Epoch 62/100, Training Loss: 0.5249, Validation Loss: 0.5286\n",
      "Epoch 63/100, Training Loss: 0.5241, Validation Loss: 0.5286\n",
      "Epoch 64/100, Training Loss: 0.5240, Validation Loss: 0.5286\n",
      "Epoch 65/100, Training Loss: 0.5241, Validation Loss: 0.5286\n",
      "Epoch 66/100, Training Loss: 0.5239, Validation Loss: 0.5286\n",
      "Epoch 00067: reducing learning rate of group 0 to 1.9683e-08.\n",
      "Epoch 67/100, Training Loss: 0.5241, Validation Loss: 0.5285\n",
      "Epoch 68/100, Training Loss: 0.5240, Validation Loss: 0.5286\n",
      "Epoch 69/100, Training Loss: 0.5242, Validation Loss: 0.5286\n",
      "Epoch 70/100, Training Loss: 0.5245, Validation Loss: 0.5286\n",
      "Epoch 71/100, Training Loss: 0.5239, Validation Loss: 0.5286\n",
      "Epoch 72/100, Training Loss: 0.5241, Validation Loss: 0.5286\n",
      "Epoch 00073: reducing learning rate of group 0 to 5.9049e-09.\n",
      "Epoch 73/100, Training Loss: 0.5245, Validation Loss: 0.5286\n",
      "Epoch 74/100, Training Loss: 0.5239, Validation Loss: 0.5286\n",
      "Epoch 75/100, Training Loss: 0.5237, Validation Loss: 0.5286\n",
      "Epoch 76/100, Training Loss: 0.5241, Validation Loss: 0.5286\n",
      "Epoch 77/100, Training Loss: 0.5245, Validation Loss: 0.5285\n",
      "Epoch 78/100, Training Loss: 0.5242, Validation Loss: 0.5285\n",
      "Epoch 79/100, Training Loss: 0.5242, Validation Loss: 0.5285\n",
      "Epoch 80/100, Training Loss: 0.5243, Validation Loss: 0.5285\n",
      "Epoch 81/100, Training Loss: 0.5241, Validation Loss: 0.5285\n",
      "Epoch 82/100, Training Loss: 0.5239, Validation Loss: 0.5285\n",
      "Epoch 83/100, Training Loss: 0.5242, Validation Loss: 0.5285\n",
      "Epoch 84/100, Training Loss: 0.5241, Validation Loss: 0.5285\n",
      "Epoch 85/100, Training Loss: 0.5240, Validation Loss: 0.5285\n",
      "Epoch 86/100, Training Loss: 0.5242, Validation Loss: 0.5285\n",
      "Epoch 87/100, Training Loss: 0.5241, Validation Loss: 0.5285\n",
      "Epoch 88/100, Training Loss: 0.5239, Validation Loss: 0.5285\n",
      "Epoch 89/100, Training Loss: 0.5240, Validation Loss: 0.5285\n",
      "Epoch 90/100, Training Loss: 0.5244, Validation Loss: 0.5286\n",
      "Epoch 91/100, Training Loss: 0.5240, Validation Loss: 0.5285\n",
      "Epoch 92/100, Training Loss: 0.5241, Validation Loss: 0.5286\n",
      "Epoch 93/100, Training Loss: 0.5247, Validation Loss: 0.5285\n",
      "Epoch 94/100, Training Loss: 0.5240, Validation Loss: 0.5285\n",
      "Epoch 95/100, Training Loss: 0.5242, Validation Loss: 0.5285\n",
      "Epoch 96/100, Training Loss: 0.5238, Validation Loss: 0.5286\n",
      "Epoch 97/100, Training Loss: 0.5240, Validation Loss: 0.5286\n",
      "Epoch 98/100, Training Loss: 0.5240, Validation Loss: 0.5286\n",
      "Epoch 99/100, Training Loss: 0.5241, Validation Loss: 0.5286\n",
      "Epoch 100/100, Training Loss: 0.5242, Validation Loss: 0.5286\n"
     ]
    }
   ],
   "source": [
    "model = CustomNetwork()\n",
    "trained_model = train_model(model, trainIn_t, trainOut_t, valIn_t, valOut_t, batch_size=100, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hot_encode(cell,compound):\n",
    "    cell_vec = np.zeros(cell_mapping.size)\n",
    "    cell_dict = {value: index for index, value in enumerate(cell_mapping)}\n",
    "\n",
    "    compound_vec = np.zeros(compound_mapping.size)\n",
    "    compound_dict = {value: index for index, value in enumerate(compound_mapping)}\n",
    "\n",
    "\n",
    "    cell_vec[cell_dict[cell]]=1\n",
    "    compound_vec[compound_dict[compound]]=1\n",
    "    vector = np.concatenate((cell_vec, compound_vec), axis = 0)\n",
    "    tensor = torch.from_numpy(vector)\n",
    "    return tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.1321, 0.2412, 0.4950,  ..., 0.4223, 0.4664, 0.5452],\n",
       "       dtype=torch.float64, grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(hot_encode(\"NK cells\", \"Clotrimazole\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        0.089716\n",
      "1        0.392768\n",
      "2       -0.776956\n",
      "3       -0.502081\n",
      "4        0.056250\n",
      "           ...   \n",
      "18206    0.132327\n",
      "18207    0.040884\n",
      "18208    0.039621\n",
      "18209   -0.185311\n",
      "18210   -0.264756\n",
      "Length: 18211, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "def get_expression(cell_type, compound_name):\n",
    "    tensor = model(hot_encode(cell_type,compound_name))\n",
    "    np_array = tensor.detach().numpy()\n",
    "    df = pd.DataFrame(np_array)\n",
    "    return unnormalize(df[0],norm_min,norm_max)\n",
    "\n",
    "\n",
    "df = get_expression(\"NK cells\", \"Clotrimazole\")\n",
    "print(df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'sample_submission.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\USU\\Dropbox\\Programn\\Single-Cell-Perturbations\\Model.ipynb Cell 12\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USU/Dropbox/Programn/Single-Cell-Perturbations/Model.ipynb#X14sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# Read the sample submission and test set ID map\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/USU/Dropbox/Programn/Single-Cell-Perturbations/Model.ipynb#X14sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m sample_submission \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39msample_submission.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USU/Dropbox/Programn/Single-Cell-Perturbations/Model.ipynb#X14sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m testDf \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mread_csv(\u001b[39m\"\u001b[39m\u001b[39mid_map.csv\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/USU/Dropbox/Programn/Single-Cell-Perturbations/Model.ipynb#X14sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m# Initialize an empty list to collect the predicted values\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[39m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    579\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(\n\u001b[0;32m   1662\u001b[0m     f,\n\u001b[0;32m   1663\u001b[0m     mode,\n\u001b[0;32m   1664\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1665\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1666\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1667\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1668\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1669\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1670\u001b[0m )\n\u001b[0;32m   1671\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\io\\common.py:859\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    854\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    855\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    856\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    857\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    858\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 859\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    860\u001b[0m             handle,\n\u001b[0;32m    861\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    862\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    863\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    864\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    865\u001b[0m         )\n\u001b[0;32m    866\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    867\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    868\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'sample_submission.csv'"
     ]
    }
   ],
   "source": [
    "# Read the sample submission and test set ID map\n",
    "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "testDf = pd.read_csv(\"id_map.csv\")\n",
    "\n",
    "# Initialize an empty list to collect the predicted values\n",
    "predicted_values = []\n",
    "\n",
    "# Loop through the test set to get the predicted values\n",
    "for idx, row in testDf.iterrows():\n",
    "    cell_type = row['cell_type']\n",
    "    sm_name = row['sm_name']\n",
    "    \n",
    "    # Call your get_expression method here\n",
    "    expression_values = get_expression(cell_type, sm_name)\n",
    "    \n",
    "    # Append the values to the list\n",
    "    predicted_values.append(expression_values)\n",
    "\n",
    "# Convert the list of predicted values to a numpy array\n",
    "predicted_values_array = np.array(predicted_values)\n",
    "\n",
    "# Replace the values in the sample submission DataFrame\n",
    "sample_submission.iloc[:, 1:] = predicted_values_array\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "sample_submission.to_csv(\"my_submission.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "row_one = copy_df.iloc[0][5:]\n",
    "row_one_output = get_expression(copy_df.iloc[0][0], copy_df.iloc[0][1])\n",
    "\n",
    "for i in range( len(row_one)):\n",
    "    print(f\"{row_one[i]}     {row_one_output[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
