{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split \n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Read the data into a pandas data frame \n",
    "df = pd.read_parquet(\"de_train.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rdkit import Chem\n",
    "from rdkit.Chem import rdFMCS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The substructure 'c1ccccc1' is found 0 times in the molecule 'CCOCC'.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def count_substructure_occurrences(target_smiles, query_smiles):\n",
    "    try:\n",
    "        # Convert SMILES to RDKit molecules\n",
    "        target_mol = Chem.MolFromSmiles(target_smiles)\n",
    "        query_mol = Chem.MolFromSmiles(query_smiles)\n",
    "\n",
    "        # Check if the molecules are valid\n",
    "        if target_mol is None or query_mol is None:\n",
    "            print(\"Error: Invalid SMILES.\")\n",
    "            return None\n",
    "\n",
    "        # Use SubstructMatch to find occurrences\n",
    "        occurrences = target_mol.GetSubstructMatches(query_mol)\n",
    "\n",
    "        return len(occurrences)\n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "target_smiles = \"CCOCC\"\n",
    "query_smiles = \"c1ccccc1\"\n",
    "occurrence_count = count_substructure_occurrences(target_smiles, query_smiles)\n",
    "\n",
    "if occurrence_count is not None:\n",
    "    print(f\"The substructure '{query_smiles}' is found {occurrence_count} times in the molecule '{target_smiles}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "functional_groups  = [\"O\", \"C=O\", \"N\", \"C(=O)N\", \"C(=O)O\", \"N=O\", \"S\", \"P([O])([O])\", \"S(=O)([O])([O])\", \"C=C\", \"C#C\", \"c1ccccc1\", \"F\", \"Cl\", \"Br\", \"I\", \"C\" ]\n",
    "functional_column_names = [(i, 'int') for i in functional_groups]\n",
    "SMILES = df[\"SMILES\"].tolist()\n",
    "\n",
    "functional_groups_block = [[count_substructure_occurrences(i,j) for j in functional_groups ] for i in SMILES]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here a few entries from the SMILES column: \n",
      "0               Clc1ccccc1C(c1ccccc1)(c1ccccc1)n1ccnc1\n",
      "1               Clc1ccccc1C(c1ccccc1)(c1ccccc1)n1ccnc1\n",
      "2               Clc1ccccc1C(c1ccccc1)(c1ccccc1)n1ccnc1\n",
      "3               Clc1ccccc1C(c1ccccc1)(c1ccccc1)n1ccnc1\n",
      "4    C[C@@H]1C[C@H]2[C@@H]3CCC4=CC(=O)C=C[C@]4(C)[C...\n",
      "5    C[C@@H]1C[C@H]2[C@@H]3CCC4=CC(=O)C=C[C@]4(C)[C...\n",
      "6    C[C@@H]1C[C@H]2[C@@H]3CCC4=CC(=O)C=C[C@]4(C)[C...\n",
      "7    C[C@@H]1C[C@H]2[C@@H]3CCC4=CC(=O)C=C[C@]4(C)[C...\n",
      "8    CC[C@H](Nc1ncnc2[nH]cnc12)c1nc2cccc(F)c2c(=O)n...\n",
      "9    CC[C@H](Nc1ncnc2[nH]cnc12)c1nc2cccc(F)c2c(=O)n...\n",
      "Name: SMILES, dtype: object.\n",
      "\n",
      "[0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 1, 0, 0, 22]\n"
     ]
    }
   ],
   "source": [
    "print(f\"Here a few entries from the SMILES column: \\n{df['SMILES'].head(10)}.\\n\")\n",
    "print(functional_groups_block[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dataframe(column_names, data):\n",
    "    df = pd.DataFrame(data, columns=[name for name, _ in column_names])\n",
    "    return df\n",
    "\n",
    "functional_group_df = create_dataframe(functional_column_names, functional_groups_block)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "#normalizes data between -1 and 1\n",
    "#returns normalized data and the factors used to normalzie \n",
    "def normalize(df):\n",
    "    min = df.min()\n",
    "    max=df.max()\n",
    "    df_normalized = (df - min) / (max-min)\n",
    "    return df_normalized, min.reset_index(drop=True), max.reset_index(drop=True)\n",
    "\n",
    "\n",
    "#unnormalize\n",
    "def unnormalize(normalized_df, min, max):\n",
    "    return min + normalized_df*(max-min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "############################DATA PREP###############################\n",
    "\n",
    "\n",
    "\n",
    "####One hot incodes inputs####\n",
    "\n",
    "# Initialize the OneHotEncoder\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "\n",
    "# Fit and transform the data\n",
    "cells = df[\"cell_type\"].values.reshape(-1, 1)\n",
    "hot_cells = encoder.fit_transform(cells)\n",
    "cell_mapping = encoder.categories_[0]\n",
    "\n",
    "compounds = df['sm_name'].values.reshape(-1, 1)\n",
    "hot_compounds = encoder.fit_transform(compounds)\n",
    "compound_mapping = encoder.categories_[0]\n",
    "\n",
    "#Puts together inputs\n",
    "hot_encoded = np.hstack((hot_cells, hot_compounds))\n",
    "hot_encoded_df = pd.DataFrame(data = hot_encoded)\n",
    "inputs_df = pd.concat([hot_encoded_df, functional_group_df], axis=1)\n",
    "\n",
    "####Normalizes Outputs####\n",
    "\n",
    "outputs = df.loc[:, 'A1BG':'ZZEF1']\n",
    "outputs_norm_df, norm_min, norm_max = normalize(outputs)\n",
    "\n",
    "\n",
    "####Puts Inputs and Outputs Together####\n",
    "prepped_df = pd.concat([inputs_df, outputs_norm_df], axis=1)\n",
    "\n",
    "print(type(hot_cells))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "#Break into training and validation and split inputs from outputs\n",
    "val, train = train_test_split(prepped_df, train_size=.2, random_state=3)\n",
    "trainIn_df = train.loc[:, 0:\"C\"]\n",
    "trainOut_df = train.loc[:, 'A1BG':'ZZEF1']\n",
    "valIn_df = val.loc[:,0:\"C\"]\n",
    "valOut_df = val.loc[:, 'A1BG':'ZZEF1']\n",
    "\n",
    "#Transforms data frames into tensors\n",
    "trainIn_t = torch.tensor(trainIn_df.values).float()\n",
    "trainOut_t = torch.tensor(trainOut_df.values).float()\n",
    "valIn_t = torch.tensor(valIn_df.values).float()\n",
    "valOut_t = torch.tensor(valOut_df.values).float()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class CustomNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomNetwork, self).__init__()\n",
    "        # Define the layers\n",
    "        self.layer1 = nn.Linear(169, 512)  # First hidden layer\n",
    "        self.batch_norm1 = nn.BatchNorm1d(512)\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "\n",
    "        self.layer2 = nn.Linear(512, 1024) # Second hidden layer\n",
    "        self.batch_norm2 = nn.BatchNorm1d(1024)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "\n",
    "        self.layer3 = nn.Linear(1024, 512) # Third hidden layer\n",
    "        self.batch_norm3 = nn.BatchNorm1d(512)\n",
    "        self.dropout3 = nn.Dropout(0.3)\n",
    "\n",
    "        self.layer4 = nn.Linear(512, 256) # Fourth hidden layer\n",
    "        self.batch_norm4 = nn.BatchNorm1d(256)\n",
    "        self.dropout4 = nn.Dropout(0.3)\n",
    "\n",
    "        self.output_layer = nn.Linear(256, 18211) # Output layer\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = F.relu(self.batch_norm1(self.layer1(x)))\n",
    "        x = self.dropout1(x)\n",
    "\n",
    "        x = F.relu(self.batch_norm2(self.layer2(x)))\n",
    "        x = self.dropout2(x)\n",
    "\n",
    "        x = F.relu(self.batch_norm3(self.layer3(x)))\n",
    "        x = self.dropout3(x)\n",
    "\n",
    "        x = F.relu(self.batch_norm4(self.layer4(x)))\n",
    "        x = self.dropout4(x)\n",
    "\n",
    "        x = self.output_layer(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "def train_model(model, trainIn_t, trainOut_t, valIn_t, valOut_t, batch_size, num_epochs):\n",
    "    criterion = nn.BCEWithLogitsLoss()  # Adjust according to your needs\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=5, factor=0.3, verbose=True)\n",
    "\n",
    "    num_samples = trainIn_t.shape[0]\n",
    "    num_batches = int(np.ceil(num_samples / batch_size))\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()  # Set the model to training mode\n",
    "        epoch_loss = 0.0\n",
    "\n",
    "        # Shuffle the data at the beginning of each epoch\n",
    "        permutation = torch.randperm(num_samples)\n",
    "\n",
    "        for batch_idx in range(num_batches):\n",
    "            start_idx = batch_idx * batch_size\n",
    "            end_idx = min(start_idx + batch_size, num_samples)\n",
    "            indices = permutation[start_idx:end_idx]\n",
    "\n",
    "            batch_trainIn = trainIn_t[indices]\n",
    "            batch_trainOut = trainOut_t[indices]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(batch_trainIn)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(outputs, batch_trainOut)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_loss += loss.item() * len(indices)\n",
    "\n",
    "        epoch_loss /= num_samples\n",
    "\n",
    "        # Validation\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_output_predictions = model(valIn_t)\n",
    "            val_loss = criterion(val_output_predictions, valOut_t)\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{num_epochs}, Training Loss: {epoch_loss:.4f}, Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "    return model\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100, Training Loss: 0.6765, Validation Loss: 0.6876\n",
      "Epoch 2/100, Training Loss: 0.5996, Validation Loss: 0.6132\n",
      "Epoch 3/100, Training Loss: 0.5581, Validation Loss: 0.5614\n",
      "Epoch 4/100, Training Loss: 0.5421, Validation Loss: 0.5390\n",
      "Epoch 5/100, Training Loss: 0.5363, Validation Loss: 0.5328\n",
      "Epoch 6/100, Training Loss: 0.5335, Validation Loss: 0.5313\n",
      "Epoch 7/100, Training Loss: 0.5310, Validation Loss: 0.5297\n",
      "Epoch 8/100, Training Loss: 0.5286, Validation Loss: 0.5293\n",
      "Epoch 9/100, Training Loss: 0.5279, Validation Loss: 0.5289\n",
      "Epoch 10/100, Training Loss: 0.5277, Validation Loss: 0.5286\n",
      "Epoch 11/100, Training Loss: 0.5271, Validation Loss: 0.5286\n",
      "Epoch 12/100, Training Loss: 0.5265, Validation Loss: 0.5280\n",
      "Epoch 13/100, Training Loss: 0.5257, Validation Loss: 0.5284\n",
      "Epoch 14/100, Training Loss: 0.5256, Validation Loss: 0.5285\n",
      "Epoch 15/100, Training Loss: 0.5256, Validation Loss: 0.5284\n",
      "Epoch 16/100, Training Loss: 0.5247, Validation Loss: 0.5286\n",
      "Epoch 17/100, Training Loss: 0.5246, Validation Loss: 0.5293\n",
      "Epoch 00018: reducing learning rate of group 0 to 3.0000e-04.\n",
      "Epoch 18/100, Training Loss: 0.5251, Validation Loss: 0.5283\n",
      "Epoch 19/100, Training Loss: 0.5250, Validation Loss: 0.5284\n",
      "Epoch 20/100, Training Loss: 0.5245, Validation Loss: 0.5284\n",
      "Epoch 21/100, Training Loss: 0.5244, Validation Loss: 0.5284\n",
      "Epoch 22/100, Training Loss: 0.5246, Validation Loss: 0.5282\n",
      "Epoch 23/100, Training Loss: 0.5254, Validation Loss: 0.5283\n",
      "Epoch 00024: reducing learning rate of group 0 to 9.0000e-05.\n",
      "Epoch 24/100, Training Loss: 0.5243, Validation Loss: 0.5283\n",
      "Epoch 25/100, Training Loss: 0.5241, Validation Loss: 0.5283\n",
      "Epoch 26/100, Training Loss: 0.5244, Validation Loss: 0.5284\n",
      "Epoch 27/100, Training Loss: 0.5241, Validation Loss: 0.5284\n",
      "Epoch 28/100, Training Loss: 0.5244, Validation Loss: 0.5285\n",
      "Epoch 29/100, Training Loss: 0.5242, Validation Loss: 0.5285\n",
      "Epoch 00030: reducing learning rate of group 0 to 2.7000e-05.\n",
      "Epoch 30/100, Training Loss: 0.5244, Validation Loss: 0.5285\n",
      "Epoch 31/100, Training Loss: 0.5242, Validation Loss: 0.5284\n",
      "Epoch 32/100, Training Loss: 0.5240, Validation Loss: 0.5284\n",
      "Epoch 33/100, Training Loss: 0.5240, Validation Loss: 0.5284\n",
      "Epoch 34/100, Training Loss: 0.5240, Validation Loss: 0.5284\n",
      "Epoch 35/100, Training Loss: 0.5243, Validation Loss: 0.5284\n",
      "Epoch 00036: reducing learning rate of group 0 to 8.1000e-06.\n",
      "Epoch 36/100, Training Loss: 0.5242, Validation Loss: 0.5285\n",
      "Epoch 37/100, Training Loss: 0.5242, Validation Loss: 0.5285\n",
      "Epoch 38/100, Training Loss: 0.5240, Validation Loss: 0.5285\n",
      "Epoch 39/100, Training Loss: 0.5241, Validation Loss: 0.5284\n",
      "Epoch 40/100, Training Loss: 0.5239, Validation Loss: 0.5285\n",
      "Epoch 41/100, Training Loss: 0.5240, Validation Loss: 0.5285\n",
      "Epoch 00042: reducing learning rate of group 0 to 2.4300e-06.\n",
      "Epoch 42/100, Training Loss: 0.5238, Validation Loss: 0.5285\n",
      "Epoch 43/100, Training Loss: 0.5238, Validation Loss: 0.5285\n",
      "Epoch 44/100, Training Loss: 0.5240, Validation Loss: 0.5285\n",
      "Epoch 45/100, Training Loss: 0.5240, Validation Loss: 0.5285\n",
      "Epoch 46/100, Training Loss: 0.5238, Validation Loss: 0.5285\n",
      "Epoch 47/100, Training Loss: 0.5241, Validation Loss: 0.5285\n",
      "Epoch 00048: reducing learning rate of group 0 to 7.2900e-07.\n",
      "Epoch 48/100, Training Loss: 0.5244, Validation Loss: 0.5285\n",
      "Epoch 49/100, Training Loss: 0.5240, Validation Loss: 0.5285\n",
      "Epoch 50/100, Training Loss: 0.5239, Validation Loss: 0.5284\n",
      "Epoch 51/100, Training Loss: 0.5243, Validation Loss: 0.5285\n",
      "Epoch 52/100, Training Loss: 0.5242, Validation Loss: 0.5285\n",
      "Epoch 53/100, Training Loss: 0.5238, Validation Loss: 0.5284\n",
      "Epoch 00054: reducing learning rate of group 0 to 2.1870e-07.\n",
      "Epoch 54/100, Training Loss: 0.5239, Validation Loss: 0.5284\n",
      "Epoch 55/100, Training Loss: 0.5243, Validation Loss: 0.5284\n",
      "Epoch 56/100, Training Loss: 0.5243, Validation Loss: 0.5284\n",
      "Epoch 57/100, Training Loss: 0.5237, Validation Loss: 0.5284\n",
      "Epoch 58/100, Training Loss: 0.5238, Validation Loss: 0.5284\n",
      "Epoch 59/100, Training Loss: 0.5238, Validation Loss: 0.5285\n",
      "Epoch 00060: reducing learning rate of group 0 to 6.5610e-08.\n",
      "Epoch 60/100, Training Loss: 0.5239, Validation Loss: 0.5285\n",
      "Epoch 61/100, Training Loss: 0.5243, Validation Loss: 0.5285\n",
      "Epoch 62/100, Training Loss: 0.5239, Validation Loss: 0.5285\n",
      "Epoch 63/100, Training Loss: 0.5244, Validation Loss: 0.5285\n",
      "Epoch 64/100, Training Loss: 0.5241, Validation Loss: 0.5285\n",
      "Epoch 65/100, Training Loss: 0.5238, Validation Loss: 0.5285\n",
      "Epoch 00066: reducing learning rate of group 0 to 1.9683e-08.\n",
      "Epoch 66/100, Training Loss: 0.5240, Validation Loss: 0.5285\n",
      "Epoch 67/100, Training Loss: 0.5239, Validation Loss: 0.5285\n",
      "Epoch 68/100, Training Loss: 0.5239, Validation Loss: 0.5285\n",
      "Epoch 69/100, Training Loss: 0.5239, Validation Loss: 0.5285\n",
      "Epoch 70/100, Training Loss: 0.5240, Validation Loss: 0.5285\n",
      "Epoch 71/100, Training Loss: 0.5240, Validation Loss: 0.5285\n",
      "Epoch 00072: reducing learning rate of group 0 to 5.9049e-09.\n",
      "Epoch 72/100, Training Loss: 0.5240, Validation Loss: 0.5285\n",
      "Epoch 73/100, Training Loss: 0.5242, Validation Loss: 0.5285\n",
      "Epoch 74/100, Training Loss: 0.5246, Validation Loss: 0.5285\n",
      "Epoch 75/100, Training Loss: 0.5240, Validation Loss: 0.5284\n",
      "Epoch 76/100, Training Loss: 0.5237, Validation Loss: 0.5285\n",
      "Epoch 77/100, Training Loss: 0.5239, Validation Loss: 0.5285\n",
      "Epoch 78/100, Training Loss: 0.5241, Validation Loss: 0.5285\n",
      "Epoch 79/100, Training Loss: 0.5240, Validation Loss: 0.5285\n",
      "Epoch 80/100, Training Loss: 0.5240, Validation Loss: 0.5285\n",
      "Epoch 81/100, Training Loss: 0.5242, Validation Loss: 0.5285\n",
      "Epoch 82/100, Training Loss: 0.5243, Validation Loss: 0.5285\n",
      "Epoch 83/100, Training Loss: 0.5238, Validation Loss: 0.5285\n",
      "Epoch 84/100, Training Loss: 0.5242, Validation Loss: 0.5285\n",
      "Epoch 85/100, Training Loss: 0.5237, Validation Loss: 0.5284\n",
      "Epoch 86/100, Training Loss: 0.5238, Validation Loss: 0.5284\n",
      "Epoch 87/100, Training Loss: 0.5237, Validation Loss: 0.5284\n",
      "Epoch 88/100, Training Loss: 0.5239, Validation Loss: 0.5284\n",
      "Epoch 89/100, Training Loss: 0.5242, Validation Loss: 0.5285\n",
      "Epoch 90/100, Training Loss: 0.5240, Validation Loss: 0.5285\n",
      "Epoch 91/100, Training Loss: 0.5240, Validation Loss: 0.5285\n",
      "Epoch 92/100, Training Loss: 0.5238, Validation Loss: 0.5285\n",
      "Epoch 93/100, Training Loss: 0.5240, Validation Loss: 0.5285\n",
      "Epoch 94/100, Training Loss: 0.5239, Validation Loss: 0.5285\n",
      "Epoch 95/100, Training Loss: 0.5244, Validation Loss: 0.5284\n",
      "Epoch 96/100, Training Loss: 0.5238, Validation Loss: 0.5285\n",
      "Epoch 97/100, Training Loss: 0.5237, Validation Loss: 0.5284\n",
      "Epoch 98/100, Training Loss: 0.5238, Validation Loss: 0.5284\n",
      "Epoch 99/100, Training Loss: 0.5237, Validation Loss: 0.5285\n",
      "Epoch 100/100, Training Loss: 0.5240, Validation Loss: 0.5284\n"
     ]
    }
   ],
   "source": [
    "model = CustomNetwork()\n",
    "trained_model = train_model(model, trainIn_t, trainOut_t, valIn_t, valOut_t, batch_size=100, num_epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hot_encode(cell,compound):\n",
    "    cell_vec = np.zeros(cell_mapping.size)\n",
    "    cell_dict = {value: index for index, value in enumerate(cell_mapping)}\n",
    "\n",
    "    compound_vec = np.zeros(compound_mapping.size)\n",
    "    compound_dict = {value: index for index, value in enumerate(compound_mapping)}\n",
    "\n",
    "    sm_names = df[\"sm_name\"].tolist()\n",
    "    functional_group_dict= dict(zip(sm_names, functional_groups_block))\n",
    "    \n",
    "\n",
    "    cell_vec[cell_dict[cell]]=1\n",
    "    compound_vec[compound_dict[compound]]=1\n",
    "    functional_group_vec = np.array(functional_group_dict[compound])\n",
    "    vector = np.concatenate((cell_vec, compound_vec, functional_group_vec), axis = 0)\n",
    "    tensor = torch.from_numpy(vector)\n",
    "    return tensor.float()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float32\n",
      "torch.float32\n"
     ]
    }
   ],
   "source": [
    "a = trainIn_t[1].reshape(1,-1)\n",
    "b = hot_encode(\"NK cells\", \"Clotrimazole\").reshape(1,-1)\n",
    "print(a.dtype)\n",
    "print(b.dtype)\n",
    "# print(type(a[0,1]))\n",
    "# print(type(b[0,1]))\n",
    "# print(a.shape)\n",
    "# print(b.shape)\n",
    "# print(a)\n",
    "# print(b)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.9072, -1.2558,  0.0329,  ..., -0.2862, -0.1279,  0.2803]],\n",
       "       grad_fn=<AddmmBackward0>)"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trained_model.eval()\n",
    "trained_model(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        -3.258689\n",
      "1        -4.067518\n",
      "2       -28.552191\n",
      "3       -17.801989\n",
      "4        -6.378904\n",
      "           ...    \n",
      "18206    -2.617712\n",
      "18207    -8.310726\n",
      "18208    -3.456731\n",
      "18209    -7.189487\n",
      "18210    -4.819105\n",
      "Length: 18211, dtype: float64\n",
      "0        22.085428\n",
      "1        14.421991\n",
      "2        27.557166\n",
      "3        27.603715\n",
      "4        35.278040\n",
      "           ...    \n",
      "18206    23.089832\n",
      "18207    10.844942\n",
      "18208     4.822008\n",
      "18209     7.826692\n",
      "18210     3.534737\n",
      "Length: 18211, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "print(norm_min)\n",
    "print(norm_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "       0          1          2          3          4          5         6       \n",
      "0 -51.594232 -27.286203 -26.708463 -38.290813 -76.910906 -74.564577 -7.274192  \\\n",
      "\n",
      "       7          8         9      ...      18201      18202     18203   \n",
      "0 -33.653929 -20.709475 -4.425873  ... -18.984896 -31.384534  1.034231  \\\n",
      "\n",
      "       18204      18205      18206      18207     18208    18209     18210  \n",
      "0 -31.431096 -42.970734 -57.971333 -12.824981 -5.826235 -9.11075 -2.477523  \n",
      "\n",
      "[1 rows x 18211 columns]\n"
     ]
    }
   ],
   "source": [
    "def get_expression(cell_type, compound_name):\n",
    "    tensor = model(hot_encode(cell_type,compound_name).reshape(1,-1))\n",
    "    np_array = tensor.detach().numpy()\n",
    "    df = pd.DataFrame(np_array)\n",
    "    return unnormalize(df,norm_min,norm_max)\n",
    "\n",
    "\n",
    "test = get_expression(\"NK cells\", \"Clotrimazole\")\n",
    "print(type(test))\n",
    "print(test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Must have equal len keys and value when setting with an iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\USU\\Dropbox\\Programn\\Single-Cell-Perturbations\\Heidicopy.ipynb Cell 19\u001b[0m line \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USU/Dropbox/Programn/Single-Cell-Perturbations/Heidicopy.ipynb#X22sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m predicted_values_array \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39marray(predicted_values)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USU/Dropbox/Programn/Single-Cell-Perturbations/Heidicopy.ipynb#X22sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m \u001b[39m# Replace the values in the sample submission DataFrame\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/USU/Dropbox/Programn/Single-Cell-Perturbations/Heidicopy.ipynb#X22sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m sample_submission\u001b[39m.\u001b[39;49miloc[:, \u001b[39m1\u001b[39;49m:] \u001b[39m=\u001b[39m predicted_values_array\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USU/Dropbox/Programn/Single-Cell-Perturbations/Heidicopy.ipynb#X22sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m \u001b[39m# Save the DataFrame to a new CSV file\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/USU/Dropbox/Programn/Single-Cell-Perturbations/Heidicopy.ipynb#X22sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m sample_submission\u001b[39m.\u001b[39mto_csv(\u001b[39m\"\u001b[39m\u001b[39mmy_submission.csv\u001b[39m\u001b[39m\"\u001b[39m, index\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\indexing.py:849\u001b[0m, in \u001b[0;36m_LocationIndexer.__setitem__\u001b[1;34m(self, key, value)\u001b[0m\n\u001b[0;32m    846\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_has_valid_setitem_indexer(key)\n\u001b[0;32m    848\u001b[0m iloc \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39miloc\u001b[39m\u001b[39m\"\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mobj\u001b[39m.\u001b[39miloc\n\u001b[1;32m--> 849\u001b[0m iloc\u001b[39m.\u001b[39;49m_setitem_with_indexer(indexer, value, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\indexing.py:1828\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1825\u001b[0m \u001b[39m# align and set the values\u001b[39;00m\n\u001b[0;32m   1826\u001b[0m \u001b[39mif\u001b[39;00m take_split_path:\n\u001b[0;32m   1827\u001b[0m     \u001b[39m# We have to operate column-wise\u001b[39;00m\n\u001b[1;32m-> 1828\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_setitem_with_indexer_split_path(indexer, value, name)\n\u001b[0;32m   1829\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1830\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_single_block(indexer, value, name)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.10_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python310\\site-packages\\pandas\\core\\indexing.py:1913\u001b[0m, in \u001b[0;36m_iLocIndexer._setitem_with_indexer_split_path\u001b[1;34m(self, indexer, value, name)\u001b[0m\n\u001b[0;32m   1910\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_setitem_single_column(ilocs[\u001b[39m0\u001b[39m], value, pi)\n\u001b[0;32m   1912\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1913\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1914\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMust have equal len keys and value \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1915\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mwhen setting with an iterable\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1916\u001b[0m         )\n\u001b[0;32m   1918\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   1919\u001b[0m     \u001b[39m# scalar value\u001b[39;00m\n\u001b[0;32m   1920\u001b[0m     \u001b[39mfor\u001b[39;00m loc \u001b[39min\u001b[39;00m ilocs:\n",
      "\u001b[1;31mValueError\u001b[0m: Must have equal len keys and value when setting with an iterable"
     ]
    }
   ],
   "source": [
    "# Read the sample submission and test set ID map\n",
    "sample_submission = pd.read_csv(\"sample_submission.csv\")\n",
    "testDf = pd.read_csv(\"id_map.csv\")\n",
    "\n",
    "# Initialize an empty list to collect the predicted values\n",
    "predicted_values = []\n",
    "\n",
    "# Loop through the test set to get the predicted values\n",
    "for idx, row in testDf.iterrows():\n",
    "    cell_type = row['cell_type']\n",
    "    sm_name = row['sm_name']\n",
    "    \n",
    "    # Call your get_expression method here\n",
    "    expression_values = get_expression(cell_type, sm_name)\n",
    "    \n",
    "    # Append the values to the list\n",
    "    predicted_values.append(expression_values)\n",
    "\n",
    "# Convert the list of predicted values to a numpy array\n",
    "predicted_values_array = np.array(predicted_values)\n",
    "\n",
    "# Replace the values in the sample submission DataFrame\n",
    "sample_submission.iloc[:, 1:] = predicted_values_array\n",
    "\n",
    "# Save the DataFrame to a new CSV file\n",
    "sample_submission.to_csv(\"my_submission.csv\", index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
